<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jiaben Chen</title>

    <meta name="author" content="Jiaben Chen">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/onepiece_logo.png" type="image/png">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Jiaben Chen
                </p>
                <p>I am an incoming Ph.D. student in Computer Science at UMass Amherst, advised by <a href="https://people.csail.mit.edu/ganchuang/">Prof. Chuang Gan</a>. My primary research interest lie in multi-modality learning and video synthesis.
                </p>
                <p>
                   I received my master's degree in Computer Science at <a href="https://cse.ucsd.edu/">CSE</a> of <a href="https://www.ucsd.edu/">University of California, San Diego</a>. During my master study, I was fortunate to be mentored by <a href="https://xiaolonw.github.io">Prof. Xiaolong Wang</a>. Starting from the summer of 2021, I worked with <a href="https://www.cis.upenn.edu/~jshi/">Prof. Jianbo Shi</a> at University of Pennsylvania as a research intern. I have also worked with <a href="https://jianghz.me">Prof. Huaizu Jiang</a> at Northeastern University. 
                </p>
                <p>
                    Before graduate study, I received my bachelor's degree in Computer Science and Technology at <a href="http://sist.shanghaitech.edu.cn">SIST</a> of <a href="https://www.shanghaitech.edu.cn/eng/">ShanghaiTech University</a>, under the supervision of 
                    <a href="https://mpl.sist.shanghaitech.edu.cn">Prof. Laurent Kneip</a> on event-based visual SLAM. During undergraduate, I have been lucky to work on video frame interpolation with <a href="https://svip-lab.github.io">Prof. Shenghua Gao</a> at ShanghaiTech in my senior year, and 
                    I also worked as a research intern on AutoML with <a href="https://cs.rice.edu/~xh37/index.html">Prof. Xia Hu</a> at Rice university in my junior year.
                </p>  
                <p style="text-align:center">
                  <a href="mailto:jic088@ucsd.edu">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=egMKh7MAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/JiabenChen">Github</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/JiabenC">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/jiaben-chen-183598200/">LinkedIn</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <img style="width:100%;max-width:100%;object-fit: cover;" alt="profile photo" src="images/boston_2023.png" class="hoverZoomLink">
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
                <p>
                  <sup>*</sup> indicates equal contributions.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/fig_sportsslomo.png" alt="clean-usnob" width="300" height="200">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://neu-vi.github.io/SportsSlomo/">
                    <span class="papertitle">SportsSloMo: A New Benchmark and Baselines for Human-centric Video Frame Interpolation</span>
                  </a>
                  <br>
                  <strong>Jiaben Chen</strong>, and <a href="https://jianghz.me">Huaizu Jiang</a>
                  <br>
                  <em>Computer Vision and Pattern Recognition Conference (CVPR)</em>, 2024
                  <br>
                  <a href="https://neu-vi.github.io/SportsSlomo/">project page</a> /
                  <a href="https://arxiv.org/abs/2308.16876">paper</a> /
                  <a href="https://github.com/neu-vi/SportsSloMo">code</a>
                  <p></p>
                  <p>In this paper, we introduce SportsSloMo, a benchmark consisting of more than 130K video clips and 1M video frames of high-resolution (â‰¥720p) slow-motion sports videos, for human-centric video frame interpolation.</p>
                </td>
              </tr>
            
            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/fig_iros2023.png" alt="clean-usnob" width="300" height="200">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://jiabenchen.github.io/revisit_event">
                    <span class="papertitle">Revisiting Event-based Video Frame Interpolation</span>
                  </a>
                  <br>
                  <strong>Jiaben Chen</strong>, 
                  Yichen Zhu,
                  <a href="https://dongzelian.com">Dongze Lian</a>,
                  <a href="https://scholar.google.com/citations?user=f7ox7CIAAAAJ&hl=en">Jiaqi Yang</a>, 
                  <a href="https://1fwang.github.io">Yifu Wang</a>,
                  <a href="https://zrrskywalker.github.io">Renrui Zhang</a>,
                  <a href="http://xinhangliu.com">Xinhang Liu</a>,
                  <a href="https://shenhanqian.github.io">Shenhan Qian</a>,
                  <a href="https://mpl.sist.shanghaitech.edu.cn/Director.html">Laurent Kneip</a>,
                  and <a href="https://svip-lab.github.io">Shenghua Gao</a>
                  <br>
                  <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2023
                  <br>
                  <a href="https://jiabenchen.github.io/revisit_event">project page</a> /
                  <a href="https://arxiv.org/abs/2307.12558">paper</a> /
                  <a href="https://www.youtube.com/watch?v=IZEmA0L-9_E">video</a>
                  <p></p>
                  <p>In this paper, we revist event-based video frame interpolation with a proxy-guided synthesis strategy and a event-guided optical flow refinement strategy.</p>
                </td>
              </tr>
            
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/fig_iquery.png" alt="clean-usnob" width="300" height="200">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://jiabenchen.github.io/iQuery/">
                    <span class="papertitle">iQuery: Instruments as Queries for Audio-Visual Sound Separation</span>
                  </a>
                  <br>
                  <strong>Jiaben Chen</strong>, 
                  <a href="https://zrrskywalker.github.io">Renrui Zhang</a>,
                  <a href="https://dongzelian.com">Dongze Lian</a>,
                  <a href="https://scholar.google.com/citations?user=f7ox7CIAAAAJ&hl=en">Jiaqi Yang</a>, 
                  <a href="https://adonis-galaxy.github.io/homepage/">Ziyao Zeng</a>,
                  and <a href="https://www.cis.upenn.edu/~jshi/">Jianbo Shi</a>
                  <br>
                  <em>Computer Vision and Pattern Recognition Conference (CVPR)</em>, 2023
                  <br>
                  <a href="https://jiabenchen.github.io/iQuery/">project page</a> /
                  <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_iQuery_Instruments_As_Queries_for_Audio-Visual_Sound_Separation_CVPR_2023_paper.pdf">paper</a> /
                  <a href="https://arxiv.org/abs/2212.03814">arXiv</a> /
                  <a href="https://www.youtube.com/watch?v=EZ9CgknV9Z4">video</a> /
                  <a href="https://github.com/JiabenChen/iQuery">code</a>
                  <p></p>
                  <p>In this paper, we re-formulate visual-sound separation task and propose Instrument as Query (iQuery) with a flexible query expansion mechanism.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/fig_rfp.png" alt="clean-usnob" width="300" height="200">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://xinhangliu.com/nerf_seg">
                    <span class="papertitle">Unsupervised Multi-View Object Segmentation Using Radiance Field Propagation</span>
                  </a>
                  <br>
                  <a href="http://xinhangliu.com">Xinhang Liu</a>,
                  <strong>Jiaben Chen</strong>, 
                  <a href="https://levenberg.github.io">Huai Yu</a>,
                  <a href="https://yuwingtai.github.io">Yu-Wing Tai</a>,
                  and <a href="https://scholar.google.com/citations?user=EWfpM74AAAAJ&hl=zh-CN">Chi-Keung Tang</a>
                  <br>
                  <em>Neural Information Processing Systems (NeurIPS)</em>, 2022
                  <br>
                  <a href="https://xinhangliu.com/nerf_seg">project page</a> /
                  <a href="https://arxiv.org/abs/2210.00489">paper</a> /
                  <a href="https://github.com/DarlingHang/radiance_field_propagation">code</a> /
                  <a href="https://drive.google.com/file/d/1gEqfFXhJm-RdDjzOBGby4kxyo7iPRtoe/view">data</a>
                  <p></p>
                  <p>In this paper, we propose radiance field propagation (RFP), a novel approach to segment objects in 3D during reconstruction given only unlabeled multi-view images of a scene.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/fig_devo.png" alt="clean-usnob" width="300" height="200">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2202.02556">
                    <span class="papertitle">DEVO: Visual Odometry in Challenging Conditions using a Stereo Event Depth Camera</span>
                  </a>
                  <br>
                  Yi-Fan Zuo<sup>*</sup>,
                  <a href="https://scholar.google.com/citations?user=f7ox7CIAAAAJ&hl=en">Jiaqi Yang</a><sup>*</sup>, 
                  <strong>Jiaben Chen</strong>, 
                  Xia Wang,
                  <a href="https://1fwang.github.io">Yifu Wang</a>,
                  and <a href="https://mpl.sist.shanghaitech.edu.cn/Director.html">Laurent Kneip</a>
                  <br>
                  <em>International Conference on Robotics and Automation (ICRA)</em>, 2022
                  <br>
                  <a href="https://arxiv.org/abs/2202.02556">paper</a>
                  <p></p>
                  <p>In this paper, we proposed a novel real-time visual odometry framework for a stereo setup of a high-resolution event and depth camera to deal with challenging conditions.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/fig_autovideo.png" alt="clean-usnob" width="300" height="200">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2108.04212">
                    <span class="papertitle">AutoVideo: An Automated Video Action Recognition System</span>
                  </a>
                  <br>
                  <a href="https://dczha.com">Daochen Zha</a><sup>*</sup>,
                  Zaid Pervaiz Bhat<sup>*</sup>,
                  Yi-Wei Chen<sup>*</sup>,
                  Yicheng Wang<sup>*</sup>,
                  Sirui Ding<sup>*</sup>,
                  <strong>Jiaben Chen</strong><sup>*</sup>,
                  Kwei-Herng Lai<sup>*</sup>,
                  Mohammad Qazim Bhat<sup>*</sup>,
                  Anmoll Kumar Jain,
                  Alfredo Costilla Reyes,
                  Na Zou,
                  and <a href="https://cs.rice.edu/~xh37/index.html">Xia Hu</a>
                  <br>
                  <em>International Joint Conference on Artificial Intelligence (IJCAI)</em>, 2022
                  <br>
                  <a href="https://arxiv.org/abs/2108.04212">paper</a> /
                  <a href="https://www.youtube.com/watch?v=BEInjBjeIuo">video</a> /
                  <a href="https://github.com/datamllab/autovideo">code</a>
                  <p></p>
                  <p>In this paper, we presented AutoVideo, a Python system for video action recognition based on Automated Machine Learning.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/fig_vector.png" alt="clean-usnob" width="300" height="200">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://star-datasets.github.io/vector/">
                    <span class="papertitle">VECtor: A Versatile Event-Centric Benchmark for Multi-Sensor SLAM</span>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=XYIKA8IAAAAJ&hl=en">Ling Gao</a><sup>*</sup>, 
                  Yuxuan Liang<sup>*</sup>,
                  <a href="https://scholar.google.com/citations?user=f7ox7CIAAAAJ&hl=en">Jiaqi Yang</a><sup>*</sup>, 
                  Shaoxun Wu,
                  Chenyu Wang,
                  <strong>Jiaben Chen</strong>, 
                  and <a href="https://mpl.sist.shanghaitech.edu.cn/Director.html">Laurent Kneip</a>
                  <br>
                  <em>Robotics and Automation Letters (RA-L)</em>, 2022
                  <br>
                  <em>International Conference on Intelligent Robots and Systems (IROS)</em>, 2022
                  <br>
                  <a href="https://arxiv.org/abs/2207.01404">paper</a> /
                  <a href="https://star-datasets.github.io/vector/">benchmark</a>
                  <p></p>
                  <p>In this paper, we proposed the first complete multi-sensor benchmark dataset containing an event-based stereo camera, a regular stereo camera, multiple depth sensors, and an inertial measurement unit.</p>
                </td>
              </tr>

          </tbody></table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
            <tr>
                <td>
                    <strong>Conference Reviewer:</strong> ECCV 2022, IROS 2022/2023, AAAI 2024.
                </td>
            </tr>
            <tr>
                <td>
                    <strong>Personal Interests:</strong>
                    <ul>
                        <li>I am a huge fan of Stephen Curry.</li>
                        <li>In my spare time, I enjoy playing basketball and FIFA.</li>
                    </ul>
                </td>
            </tr>
          </tbody></table>
            
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td style="text-align:center;">
                <!-- RevolverMaps globe -->
                <script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=5mu02nbkyzf&amp;m=7&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80" async="async"></script>
              </td>
            </tr>
            <tr>
              <td style="text-align:center;">
                <!-- Last update info -->
                <p style="font-size:small;">Last update: Oct, 2023</p>
              </td>
            </tr>
        </tbody></table> 
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Design and source code from <a href="https://jonbarron.info/">Jon Barron's website</a>. 
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
