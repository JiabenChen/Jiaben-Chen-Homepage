<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jiaben Chen</title>

    <meta name="author" content="Jiaben Chen">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/onepiece_logo.png" type="image/png">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Jiaben Chen
                </p>
                <p>I am a first-year Ph.D. student in Computer Science at <a href="https://www.umass.edu">UMass Amherst</a>, advised by <a href="https://people.csail.mit.edu/ganchuang/">Prof. Chuang Gan</a>. My primary research interest lies in multi-modality learning and video synthesis.
                </p>
                <p>
                   I received my master's degree in Computer Science at <a href="https://cse.ucsd.edu/">CSE</a> of <a href="https://www.ucsd.edu/">University of California, San Diego</a>, where I was mentored by <a href="https://xiaolonw.github.io">Prof. Xiaolong Wang</a>. Starting from the summer of 2021, I had the privilege of working with <a href="https://www.cis.upenn.edu/~jshi/">Prof. Jianbo Shi</a> at University of Pennsylvania as a research intern. 
                </p>
                <p>
                    Before graduate study, I received my bachelor's degree in Computer Science and Technology at <a href="http://sist.shanghaitech.edu.cn">SIST</a> of <a href="https://www.shanghaitech.edu.cn/eng/">ShanghaiTech University</a>. Throughout my academic journey, I have been fortunate to collaborate with <a href="https://jianghz.me">Prof. Huaizu Jiang</a>, <a href="https://mpl.sist.shanghaitech.edu.cn">Prof. Laurent Kneip</a>, <a href="https://scholar.google.com.sg/citations?user=fe-1v0MAAAAJ&hl=en">Prof. Shenghua Gao</a>, and <a href="https://cs.rice.edu/~xh37/index.html">Prof. Xia Hu</a>. </p>
                </p>  
                <p style="text-align:center">
                  <a href="mailto:jiabenchen@umass.edu">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=egMKh7MAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/JiabenChen">Github</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/JiabenC">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/jiaben-chen-183598200/">LinkedIn</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <img style="width:100%;max-width:100%;object-fit: cover;" alt="profile photo" src="images/boston_2023.png" class="hoverZoomLink">
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
                <p>
                  <sup>*</sup> indicates equal contributions.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/fig_rapverse.png" alt="clean-usnob" width="300" height="200">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://vis-www.cs.umass.edu/RapVerse/">
                    <span class="papertitle">RapVerse: Coherent Vocals and Whole-Body Motions Generations from Text</span>
                  </a>
                  <br>
                  <strong>Jiaben Chen</strong>,
                  <a href="https://cakeyan.github.io/">Xin Yan</a>,
                  Yihang Chen,
                  Siyuan Cen,
                  Qinwei Ma,
                  <a href="https://haoyuzhen.com/">Haoyu Zhen</a>,
                  <a href="https://scholar.google.com/citations?user=uEpr4C4AAAAJ">Kaizhi Qian</a>,
                  <a href="https://scholar.google.com/citations?user=EEds7hMAAAAJ">Lie Lu</a>,
                  and <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>
                  <br>
                  <em>arXiv Preprint</em>, 2024
                  <br>
                  <a href="https://vis-www.cs.umass.edu/RapVerse/">project page</a> /
                  <a href="https://arxiv.org/abs/2405.20336">paper</a> /
                  <a href="https://github.com/UMass-Foundation-Model/RapVerse">code</a>
                  <p></p>
                  <p>In this paper, we introduce a challenging task for simultaneously generating 3D holistic body motions and singing vocals directly from textual lyrics inputs. To facilitate this, we first collect the RapVerse dataset, a large dataset containing synchronous rapping vocals, lyrics, and high-quality 3D holistic body meshes.</p>
                </td>
              </tr>
            
            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/fig_sportsslomo.png" alt="clean-usnob" width="300" height="200">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://neu-vi.github.io/SportsSlomo/">
                    <span class="papertitle">SportsSloMo: A New Benchmark and Baselines for Human-centric Video Frame Interpolation</span>
                  </a>
                  <br>
                  <strong>Jiaben Chen</strong>, and <a href="https://jianghz.me">Huaizu Jiang</a>
                  <br>
                  <em>Computer Vision and Pattern Recognition Conference (CVPR)</em>, 2024
                  <br>
                  <a href="https://neu-vi.github.io/SportsSlomo/">project page</a> /
                  <a href="https://arxiv.org/abs/2308.16876">paper</a> /
                  <a href="https://github.com/neu-vi/SportsSloMo">code</a>
                  <p></p>
                  <p>In this paper, we introduce SportsSloMo, a benchmark consisting of more than 130K video clips and 1M video frames of high-resolution (â‰¥720p) slow-motion sports videos, for human-centric video frame interpolation.</p>
                </td>
              </tr>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/fig_deceptive.png" alt="clean-usnob" width="300" height="200">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://xinhangliu.com/deceptive-nerf-3dgs">
                    <span class="papertitle">Deceptive-NeRF/3DGS: Diffusion-Generated Pseudo-Observations for High-Quality Sparse-View Reconstruction</span>
                  </a>
                  <br>
                  <a href="http://xinhangliu.com">Xinhang Liu</a>,
                  <strong>Jiaben Chen</strong>, 
                  <a href="https://cse.hkust.edu.hk/~skao/">Shiu-Hong Kao</a>,
                  <a href="https://yuwingtai.github.io/">Yu-Wing Tai</a>,
                  and <a href="https://cse.hkust.edu.hk/~cktang/bio.html">Chi-Keung Tang</a>
                  <br>
                  <em>European Conference on Computer Vision (ECCV)</em>, 2024
                  <br>
                  <a href="https://xinhangliu.com/deceptive-nerf-3dgs">project page</a> /
                  <a href="https://arxiv.org/abs/2305.15171">paper</a> /
                  <p></p>
                  <p>In this work, we enhance sparse-view reconstruction by leveraging a diffusion model pre-trained from multiview datasets to synthesize pseudo-observations.</p>
                </td>
              </tr>

            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/fig_robodreamer.png" alt="clean-usnob" width="300" height="200">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://robovideo.github.io/">
                    <span class="papertitle">RoboDreamer: Learning Compositional World Models for Robot Imagination</span>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=WjUmtm0AAAAJ">Siyuan Zhou</a>,
                  <a href="https://yilundu.github.io/">Yilun Du</a>,
                  <strong>Jiaben Chen</strong>,
                  <a href="https://cold-winter.github.io/">Yandong Li</a>,
                  <a href="https://scholar.google.com/citations?user=nEsOOx8AAAAJ&hl=en">Dit-Yan Yeung</a>,
                  and <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>
                  <br>
                  <em>International Conference on Machine Learning (ICML)</em>, 2024
                  <br>
                  <a href="https://robovideo.github.io/">project page</a> /
                  <a href="https://arxiv.org/abs/2404.12377">paper</a> /
                  <a href="https://github.com/rainbow979/robodreamer">code</a>
                  <p></p>
                  <p>In this paper, we introduce RoboDreamer, an innovative approach for learning a compositional world model by factorizing the video generation.</p>
                </td>
              </tr>
            
            <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/fig_iros2023.png" alt="clean-usnob" width="300" height="200">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://jiabenchen.github.io/revisit_event">
                    <span class="papertitle">Revisiting Event-based Video Frame Interpolation</span>
                  </a>
                  <br>
                  <strong>Jiaben Chen</strong>, 
                  Yichen Zhu,
                  <a href="https://dongzelian.com">Dongze Lian</a>,
                  <a href="https://scholar.google.com/citations?user=f7ox7CIAAAAJ&hl=en">Jiaqi Yang</a>, 
                  <a href="https://1fwang.github.io">Yifu Wang</a>,
                  <a href="https://zrrskywalker.github.io">Renrui Zhang</a>,
                  <a href="http://xinhangliu.com">Xinhang Liu</a>,
                  <a href="https://shenhanqian.github.io">Shenhan Qian</a>,
                  <a href="https://mpl.sist.shanghaitech.edu.cn/Director.html">Laurent Kneip</a>,
                  and <a href="https://svip-lab.github.io">Shenghua Gao</a>
                  <br>
                  <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2023
                  <br>
                  <a href="https://jiabenchen.github.io/revisit_event">project page</a> /
                  <a href="https://arxiv.org/abs/2307.12558">paper</a> /
                  <a href="https://www.youtube.com/watch?v=IZEmA0L-9_E">video</a>
                  <p></p>
                  <p>In this paper, we revist event-based video frame interpolation with a proxy-guided synthesis strategy and a event-guided optical flow refinement strategy.</p>
                </td>
              </tr>
            
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/fig_iquery.png" alt="clean-usnob" width="300" height="200">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://jiabenchen.github.io/iQuery/">
                    <span class="papertitle">iQuery: Instruments as Queries for Audio-Visual Sound Separation</span>
                  </a>
                  <br>
                  <strong>Jiaben Chen</strong>, 
                  <a href="https://zrrskywalker.github.io">Renrui Zhang</a>,
                  <a href="https://dongzelian.com">Dongze Lian</a>,
                  <a href="https://scholar.google.com/citations?user=f7ox7CIAAAAJ&hl=en">Jiaqi Yang</a>, 
                  <a href="https://adonis-galaxy.github.io/homepage/">Ziyao Zeng</a>,
                  and <a href="https://www.cis.upenn.edu/~jshi/">Jianbo Shi</a>
                  <br>
                  <em>Computer Vision and Pattern Recognition Conference (CVPR)</em>, 2023
                  <br>
                  <a href="https://jiabenchen.github.io/iQuery/">project page</a> /
                  <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_iQuery_Instruments_As_Queries_for_Audio-Visual_Sound_Separation_CVPR_2023_paper.pdf">paper</a> /
                  <a href="https://arxiv.org/abs/2212.03814">arXiv</a> /
                  <a href="https://www.youtube.com/watch?v=EZ9CgknV9Z4">video</a> /
                  <a href="https://github.com/JiabenChen/iQuery">code</a>
                  <p></p>
                  <p>In this paper, we re-formulate visual-sound separation task and propose Instrument as Query (iQuery) with a flexible query expansion mechanism.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/fig_rfp.png" alt="clean-usnob" width="300" height="200">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://xinhangliu.com/nerf_seg">
                    <span class="papertitle">Unsupervised Multi-View Object Segmentation Using Radiance Field Propagation</span>
                  </a>
                  <br>
                  <a href="http://xinhangliu.com">Xinhang Liu</a>,
                  <strong>Jiaben Chen</strong>, 
                  <a href="https://levenberg.github.io">Huai Yu</a>,
                  <a href="https://yuwingtai.github.io">Yu-Wing Tai</a>,
                  and <a href="https://scholar.google.com/citations?user=EWfpM74AAAAJ&hl=zh-CN">Chi-Keung Tang</a>
                  <br>
                  <em>Neural Information Processing Systems (NeurIPS)</em>, 2022
                  <br>
                  <a href="https://xinhangliu.com/nerf_seg">project page</a> /
                  <a href="https://arxiv.org/abs/2210.00489">paper</a> /
                  <a href="https://github.com/DarlingHang/radiance_field_propagation">code</a> /
                  <a href="https://drive.google.com/file/d/1gEqfFXhJm-RdDjzOBGby4kxyo7iPRtoe/view">data</a>
                  <p></p>
                  <p>In this paper, we propose radiance field propagation (RFP), a novel approach to segment objects in 3D during reconstruction given only unlabeled multi-view images of a scene.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/fig_devo.png" alt="clean-usnob" width="300" height="200">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2202.02556">
                    <span class="papertitle">DEVO: Visual Odometry in Challenging Conditions using a Stereo Event Depth Camera</span>
                  </a>
                  <br>
                  Yi-Fan Zuo<sup>*</sup>,
                  <a href="https://scholar.google.com/citations?user=f7ox7CIAAAAJ&hl=en">Jiaqi Yang</a><sup>*</sup>, 
                  <strong>Jiaben Chen</strong>, 
                  Xia Wang,
                  <a href="https://1fwang.github.io">Yifu Wang</a>,
                  and <a href="https://mpl.sist.shanghaitech.edu.cn/Director.html">Laurent Kneip</a>
                  <br>
                  <em>International Conference on Robotics and Automation (ICRA)</em>, 2022
                  <br>
                  <a href="https://arxiv.org/abs/2202.02556">paper</a>
                  <p></p>
                  <p>In this paper, we proposed a novel real-time visual odometry framework for a stereo setup of a high-resolution event and depth camera to deal with challenging conditions.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/fig_autovideo.png" alt="clean-usnob" width="300" height="200">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2108.04212">
                    <span class="papertitle">AutoVideo: An Automated Video Action Recognition System</span>
                  </a>
                  <br>
                  <a href="https://dczha.com">Daochen Zha</a><sup>*</sup>,
                  Zaid Pervaiz Bhat<sup>*</sup>,
                  Yi-Wei Chen<sup>*</sup>,
                  Yicheng Wang<sup>*</sup>,
                  Sirui Ding<sup>*</sup>,
                  <strong>Jiaben Chen</strong><sup>*</sup>,
                  Kwei-Herng Lai<sup>*</sup>,
                  Mohammad Qazim Bhat<sup>*</sup>,
                  Anmoll Kumar Jain,
                  Alfredo Costilla Reyes,
                  Na Zou,
                  and <a href="https://cs.rice.edu/~xh37/index.html">Xia Hu</a>
                  <br>
                  <em>International Joint Conference on Artificial Intelligence (IJCAI)</em>, 2022
                  <br>
                  <a href="https://arxiv.org/abs/2108.04212">paper</a> /
                  <a href="https://www.youtube.com/watch?v=BEInjBjeIuo">video</a> /
                  <a href="https://github.com/datamllab/autovideo">code</a>
                  <p></p>
                  <p>In this paper, we presented AutoVideo, a Python system for video action recognition based on Automated Machine Learning.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/fig_vector.png" alt="clean-usnob" width="300" height="200">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://star-datasets.github.io/vector/">
                    <span class="papertitle">VECtor: A Versatile Event-Centric Benchmark for Multi-Sensor SLAM</span>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=XYIKA8IAAAAJ&hl=en">Ling Gao</a><sup>*</sup>, 
                  Yuxuan Liang<sup>*</sup>,
                  <a href="https://scholar.google.com/citations?user=f7ox7CIAAAAJ&hl=en">Jiaqi Yang</a><sup>*</sup>, 
                  Shaoxun Wu,
                  Chenyu Wang,
                  <strong>Jiaben Chen</strong>, 
                  and <a href="https://mpl.sist.shanghaitech.edu.cn/Director.html">Laurent Kneip</a>
                  <br>
                  <em>Robotics and Automation Letters (RA-L)</em>, 2022
                  <br>
                  <em>International Conference on Intelligent Robots and Systems (IROS)</em>, 2022
                  <br>
                  <a href="https://arxiv.org/abs/2207.01404">paper</a> /
                  <a href="https://star-datasets.github.io/vector/">benchmark</a>
                  <p></p>
                  <p>In this paper, we proposed the first complete multi-sensor benchmark dataset containing an event-based stereo camera, a regular stereo camera, multiple depth sensors, and an inertial measurement unit.</p>
                </td>
              </tr>

          </tbody></table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
            <tr>
                <td>
                    <strong>Conference Reviewer:</strong> ECCV 2022, IROS 2022/2023, AAAI 2024.
                </td>
            </tr>
            <tr>
                <td>
                    <strong>Personal Interests:</strong>
                    <ul>
                        <li>I am a huge fan of Stephen Curry.</li>
                        <li>In my spare time, I enjoy playing basketball and FIFA.</li>
                    </ul>
                </td>
            </tr>
          </tbody></table>
            
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td style="text-align:center;">
                <!-- RevolverMaps globe -->
                <script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=5mu02nbkyzf&amp;m=7&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80" async="async"></script>
              </td>
            </tr>
            <tr>
              <td style="text-align:center;">
                <!-- Last update info -->
                <p style="font-size:small;">Last update: Oct, 2023</p>
              </td>
            </tr>
        </tbody></table> 
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Design and source code from <a href="https://jonbarron.info/">Jon Barron's website</a>. 
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
